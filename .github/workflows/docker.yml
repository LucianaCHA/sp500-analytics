name: Docker CI/CD

on:
  push:
    branches: [ main ]
    paths:
      - 'airflow/**'
      - 'dags/**'
      - 'scripts/**'
      - 'infra/**'
      - 'dashboard/**'
      - '**/informe_app.py'
      - 'Dockerfile'
      - 'requirements.txt'
      - 'config.py'
      - 'docker-compose.yml'
      - '.github/workflows/docker.yml'
  workflow_dispatch:

env:
  IMAGE_NAME: sp500-etl
  BUILD_CONTEXT: .
  DOCKERFILE_PATH: ./Dockerfile

jobs:
  # === BUILD Y PUSH DE ETL ===
  build-and-push-etl:
    name: Build and Push ETL Docker Image
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Build ETL image
        run: |
          unset DOCKER_CONTEXT || true
          docker build \
            -f "${{ env.DOCKERFILE_PATH }}" \
            -t "${{ secrets.DOCKERHUB_USERNAME }}/${{ env.IMAGE_NAME }}:latest" \
            "${{ env.BUILD_CONTEXT }}"

      - name: Push ETL image
        run: docker push "${{ secrets.DOCKERHUB_USERNAME }}/${{ env.IMAGE_NAME }}:latest"

  # === BUILD Y PUSH DE STREAMLIT ===
  build-and-push-streamlit:
    name: Build and Push Streamlit Docker Image
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Build Streamlit image
        run: |
          unset DOCKER_CONTEXT || true
          docker build \
            -f ./dashboard/streamlit_app/Dockerfile \
            -t "${{ secrets.DOCKERHUB_USERNAME }}/sp500-streamlit:latest" \
            ./dashboard/streamlit_app

      - name: Push Streamlit image
        run: docker push "${{ secrets.DOCKERHUB_USERNAME }}/sp500-streamlit:latest"

  # === DEPLOY DE AIRFLOW/ETL EN EC2 ===
  deploy-etl-to-ec2:
    name: Deploy ETL/Airflow to EC2
    needs: build-and-push-etl
    runs-on: ubuntu-latest
    environment: prod

    env:
      # Vars (no sensibles)
      AWS_DB_USER: ${{ vars.AWS_DB_USER }}
      AWS_DB_HOST: ${{ vars.AWS_DB_HOST }}
      AWS_DB_PORT: ${{ vars.AWS_DB_PORT }}
      AWS_DB_NAME: ${{ vars.AWS_DB_NAME }}
      AWS_DEFAULT_REGION: ${{ vars.AWS_DEFAULT_REGION }}
      S3_BUCKET: ${{ vars.S3_BUCKET }}

      # Secrets (sensibles)
      AWS_DB_PASSWORD: ${{ secrets.AWS_DB_PASSWORD }}
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AIRFLOW__CORE__FERNET_KEY: ${{ secrets.AIRFLOW__CORE__FERNET_KEY }}
      AIRFLOW__WEBSERVER__SECRET_KEY: ${{ secrets.AIRFLOW__WEBSERVER__SECRET_KEY }}

    steps:
      - uses: actions/checkout@v4

      - name: Setup SSH key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.EC2_SSH_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          ssh-keyscan -H "${{ secrets.EC2_HOST }}" >> ~/.ssh/known_hosts 2>/dev/null || true

      - name: Debug (safe)
        run: |
          echo "REGION='${{ env.AWS_DEFAULT_REGION }}'"
          echo "BUCKET='${{ env.S3_BUCKET }}'"
          test -n "${{ env.AWS_ACCESS_KEY_ID }}" && echo "AWS_ACCESS_KEY_ID OK" || echo "AWS_ACCESS_KEY_ID VACIO"
          test -n "${{ env.AIRFLOW__WEBSERVER__SECRET_KEY }}" && echo "WEBSERVER_SECRET_KEY OK" || echo "WEBSERVER_SECRET_KEY VACIO"
          test -n "${{ env.AIRFLOW__CORE__FERNET_KEY }}" && echo "FERNET_KEY OK" || echo "FERNET_KEY VACIO"

      - name: Deploy ETL/Airflow to EC2
        run: |
          ssh ${{ secrets.EC2_USER }}@${{ secrets.EC2_HOST }} << 'EOF'
            set -e

            # === Inyectar envs del runner al host remoto (EC2) ===
            export AWS_DB_USER='${{ env.AWS_DB_USER }}'
            export AWS_DB_PASSWORD='${{ env.AWS_DB_PASSWORD }}'
            export AWS_DB_HOST='${{ env.AWS_DB_HOST }}'
            export AWS_DB_PORT='${{ env.AWS_DB_PORT }}'
            export AWS_DB_NAME='${{ env.AWS_DB_NAME }}'
            export AWS_ACCESS_KEY_ID='${{ env.AWS_ACCESS_KEY_ID }}'
            export AWS_SECRET_ACCESS_KEY='${{ env.AWS_SECRET_ACCESS_KEY }}'
            export AWS_DEFAULT_REGION='${{ env.AWS_DEFAULT_REGION }}'
            export AWS_REGION='${{ env.AWS_DEFAULT_REGION }}'
            export S3_BUCKET='${{ env.S3_BUCKET }}'
            export AIRFLOW__CORE__FERNET_KEY='${{ env.AIRFLOW__CORE__FERNET_KEY }}'
            export AIRFLOW__WEBSERVER__SECRET_KEY='${{ env.AIRFLOW__WEBSERVER__SECRET_KEY }}'

            # Sanitizar dockerhub username (por si tiene newline)
            DOCKERHUB_USER='${{ secrets.DOCKERHUB_USERNAME }}'
            DOCKERHUB_USER="$(echo -n "$DOCKERHUB_USER" | tr -d '\r\n' | xargs)"

            IMAGE="${DOCKERHUB_USER}/${{ env.IMAGE_NAME }}:latest"
            echo "IMAGE=[$IMAGE]" | cat -A

            sudo mkdir -p /opt/sp500
            cd /opt/sp500

            sudo systemctl start docker || true
            sudo docker network create airflow-net 2>/dev/null || true

            # --- Postgres (volumen persistente) ---
            sudo docker rm -f airflow-postgres 2>/dev/null || true
            sudo docker run -d \
              --name airflow-postgres \
              --network airflow-net \
              --restart unless-stopped \
              -e POSTGRES_USER=airflow \
              -e POSTGRES_PASSWORD=airflow \
              -e POSTGRES_DB=airflow \
              -v airflow_pgdata:/var/lib/postgresql/data \
              -p 5432:5432 \
              postgres:15

            # --- Stop/RM contenedores ETL/Airflow ---
            for c in sp500-etl airflow-scheduler airflow-triggerer; do
              sudo docker rm -f "$c" 2>/dev/null || true
            done

            sudo docker container prune -f || true
            sudo docker image prune -a -f || true
            sudo docker builder prune -a -f || true

            sudo docker pull "$IMAGE"

            AIRFLOW_DB_URI="postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow"

            # MigraciÃ³n DB
            sudo docker run --rm \
              --network airflow-net \
              -e AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$AIRFLOW_DB_URI" \
              -e AIRFLOW__CORE__FERNET_KEY="$AIRFLOW__CORE__FERNET_KEY" \
              -e AIRFLOW__WEBSERVER__SECRET_KEY="$AIRFLOW__WEBSERVER__SECRET_KEY" \
              -e AIRFLOW__CORE__EXECUTOR=LocalExecutor \
              -e SP500_URL="https://raw.githubusercontent.com/lucianabuzzo/sp500-analytics/main/data/sp500_companies.csv" \
              "$IMAGE" airflow db migrate

            # Webserver
            sudo docker run -d \
              --name sp500-etl \
              --network airflow-net \
              --restart unless-stopped \
              -p 8080:8080 \
              --log-driver awslogs \
              --log-opt awslogs-region="$AWS_DEFAULT_REGION" \
              --log-opt awslogs-group=/sp500/etl \
              --log-opt awslogs-stream=sp500-etl-webserver \
              -e AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$AIRFLOW_DB_URI" \
              -e AIRFLOW__CORE__FERNET_KEY="$AIRFLOW__CORE__FERNET_KEY" \
              -e AIRFLOW__WEBSERVER__SECRET_KEY="$AIRFLOW__WEBSERVER__SECRET_KEY" \
              -e AWS_DB_USER="$AWS_DB_USER" \
              -e AWS_DB_PASSWORD="$AWS_DB_PASSWORD" \
              -e AWS_DB_HOST="$AWS_DB_HOST" \
              -e AWS_DB_PORT="$AWS_DB_PORT" \
              -e AWS_DB_NAME="$AWS_DB_NAME" \
              -e AWS_ACCESS_KEY_ID="$AWS_ACCESS_KEY_ID" \
              -e AWS_SECRET_ACCESS_KEY="$AWS_SECRET_ACCESS_KEY" \
              -e AWS_DEFAULT_REGION="$AWS_DEFAULT_REGION" \
              -e AWS_REGION="$AWS_REGION" \
              -e S3_BUCKET="$S3_BUCKET" \
              -e AIRFLOW__CORE__EXECUTOR=LocalExecutor \
              -e SP500_URL="https://raw.githubusercontent.com/lucianabuzzo/sp500-analytics/main/data/sp500_companies.csv" \
              -e ENVIRONMENT=prod \
              "$IMAGE" webserver

            # Scheduler
            sudo docker run -d \
              --name airflow-scheduler \
              --network airflow-net \
              --restart unless-stopped \
              -e AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$AIRFLOW_DB_URI" \
              -e AIRFLOW__CORE__FERNET_KEY="$AIRFLOW__CORE__FERNET_KEY" \
              -e AIRFLOW__WEBSERVER__SECRET_KEY="$AIRFLOW__WEBSERVER__SECRET_KEY" \
              -e AIRFLOW__CORE__EXECUTOR=LocalExecutor \
              -e AWS_ACCESS_KEY_ID="$AWS_ACCESS_KEY_ID" \
              -e AWS_SECRET_ACCESS_KEY="$AWS_SECRET_ACCESS_KEY" \
              -e AWS_DEFAULT_REGION="$AWS_DEFAULT_REGION" \
              -e AWS_REGION="$AWS_REGION" \
              -e S3_BUCKET="$S3_BUCKET" \
              -e SP500_URL="https://raw.githubusercontent.com/lucianabuzzo/sp500-analytics/main/data/sp500_companies.csv" \
              -e ENVIRONMENT=prod \
              "$IMAGE" scheduler

            # Triggerer
            sudo docker run -d \
              --name airflow-triggerer \
              --network airflow-net \
              --restart unless-stopped \
              -e AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$AIRFLOW_DB_URI" \
              -e AIRFLOW__CORE__FERNET_KEY="$AIRFLOW__CORE__FERNET_KEY" \
              -e AIRFLOW__WEBSERVER__SECRET_KEY="$AIRFLOW__WEBSERVER__SECRET_KEY" \
              -e AIRFLOW__CORE__EXECUTOR=LocalExecutor \
              -e AWS_ACCESS_KEY_ID="$AWS_ACCESS_KEY_ID" \
              -e AWS_SECRET_ACCESS_KEY="$AWS_SECRET_ACCESS_KEY" \
              -e AWS_DEFAULT_REGION="$AWS_DEFAULT_REGION" \
              -e AWS_REGION="$AWS_REGION" \
              -e S3_BUCKET="$S3_BUCKET" \
              -e SP500_URL="https://raw.githubusercontent.com/lucianabuzzo/sp500-analytics/main/data/sp500_companies.csv" \
              -e ENVIRONMENT=prod \
              "$IMAGE" triggerer
          EOF

  # === DEPLOY DE STREAMLIT EN EC2 ===
  deploy-streamlit-to-ec2:
    name: Deploy Streamlit to EC2
    needs: build-and-push-streamlit
    runs-on: ubuntu-latest
    environment: prod

    env:
      AWS_DEFAULT_REGION: ${{ vars.AWS_DEFAULT_REGION }}
      S3_BUCKET: ${{ vars.S3_BUCKET }}
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

    steps:
      - uses: actions/checkout@v4

      - name: Setup SSH key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.EC2_SSH_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          ssh-keyscan -H "${{ secrets.EC2_HOST }}" >> ~/.ssh/known_hosts 2>/dev/null || true

      - name: Deploy Streamlit to EC2
        run: |
          ssh ${{ secrets.EC2_USER }}@${{ secrets.EC2_HOST }} << 'EOF'
            set -e

            export AWS_DEFAULT_REGION='${{ env.AWS_DEFAULT_REGION }}'
            export AWS_REGION='${{ env.AWS_DEFAULT_REGION }}'
            export S3_BUCKET='${{ env.S3_BUCKET }}'
            export AWS_ACCESS_KEY_ID='${{ env.AWS_ACCESS_KEY_ID }}'
            export AWS_SECRET_ACCESS_KEY='${{ env.AWS_SECRET_ACCESS_KEY }}'

            DOCKERHUB_USER='${{ secrets.DOCKERHUB_USERNAME }}'
            DOCKERHUB_USER="$(echo -n "$DOCKERHUB_USER" | tr -d '\r\n' | xargs)"

            IMAGE="${DOCKERHUB_USER}/sp500-streamlit:latest"
            echo "IMAGE=[$IMAGE]" | cat -A

            sudo mkdir -p /opt/sp500
            cd /opt/sp500

            sudo systemctl start docker || true

            for id in $(sudo docker ps --filter "publish=8501" -q); do
              sudo docker rm -f "$id" 2>/dev/null || true
            done

            sudo docker rm -f sp500-streamlit streamlit-app 2>/dev/null || true

            sudo docker container prune -f || true
            sudo docker image prune -a -f || true
            sudo docker builder prune -a -f || true

            sudo docker pull "$IMAGE"

            sudo docker run -d \
              --name sp500-streamlit \
              --restart unless-stopped \
              -p 8501:8501 \
              --log-driver awslogs \
              --log-opt awslogs-region="$AWS_DEFAULT_REGION" \
              --log-opt awslogs-group=/sp500/etl \
              --log-opt awslogs-stream=sp500-streamlit \
              -e AWS_ACCESS_KEY_ID="$AWS_ACCESS_KEY_ID" \
              -e AWS_SECRET_ACCESS_KEY="$AWS_SECRET_ACCESS_KEY" \
              -e AWS_DEFAULT_REGION="$AWS_DEFAULT_REGION" \
              -e AWS_REGION="$AWS_REGION" \
              -e S3_BUCKET="$S3_BUCKET" \
              "$IMAGE"
          EOF
name: Docker CI/CD

on:
  push:
    branches:
      - main
    paths:
      - 'airflow/**'
      - 'dags/**'
      - 'scripts/**'
      - 'infra/**'
      - 'dashboard/**'
      - '**/informe_app.py'
      - 'Dockerfile'
      - 'requirements.txt'
      - 'config.py'
      - 'docker-compose.yml'
      - '.github/workflows/docker.yml'
  workflow_dispatch:

env:
  IMAGE_NAME: sp500-etl
  BUILD_CONTEXT: .
  DOCKERFILE_PATH: ./Dockerfile

jobs:
  # === BUILD Y PUSH DE ETL ===
  build-and-push-etl:
    name: Build and Push ETL Docker Image
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Build ETL image
        run: |
          unset DOCKER_CONTEXT || true
          docker build \
            -f "${{ env.DOCKERFILE_PATH }}" \
            -t "${{ secrets.DOCKERHUB_USERNAME }}/${{ env.IMAGE_NAME }}:latest" \
            "${{ env.BUILD_CONTEXT }}"

      - name: Push ETL image
        run: |
          docker push "${{ secrets.DOCKERHUB_USERNAME }}/${{ env.IMAGE_NAME }}:latest"

  # === BUILD Y PUSH DE STREAMLIT ===
  build-and-push-streamlit:
    name: Build and Push Streamlit Docker Image
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Build Streamlit image
        run: |
          unset DOCKER_CONTEXT || true
          docker build \
            -f ./dashboard/streamlit_app/Dockerfile \
            -t "${{ secrets.DOCKERHUB_USERNAME }}/sp500-streamlit:latest" \
            ./dashboard/streamlit_app

      - name: Push Streamlit image
        run: |
          docker push "${{ secrets.DOCKERHUB_USERNAME }}/sp500-streamlit:latest"

  # === DEPLOY DE AIRFLOW/ETL EN EC2 ===
  deploy-etl-to-ec2:
    environment: prod
    name: Deploy ETL/Airflow to EC2
    needs: build-and-push-etl
    runs-on: ubuntu-latest

    env:
      AWS_DB_USER: ${{ vars.AWS_DB_USER }}
      AWS_DB_PASSWORD: ${{ vars.AWS_DB_PASSWORD }}
      AWS_DB_HOST: ${{ vars.AWS_DB_HOST }}
      AWS_DB_PORT: ${{ vars.AWS_DB_PORT }}
      AWS_DB_NAME: ${{ vars.AWS_DB_NAME }}
      AWS_ACCESS_KEY_ID: ${{ vars.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ vars.AWS_SECRET_ACCESS_KEY }}
      AWS_DEFAULT_REGION: ${{ vars.AWS_DEFAULT_REGION }}
      S3_BUCKET: ${{ vars.S3_BUCKET }}
      AIRFLOW__CORE__FERNET_KEY: ${{ vars.AIRFLOW__CORE__FERNET_KEY }}
      AIRFLOW__WEBSERVER__SECRET_KEY: ${{ vars.AIRFLOW__WEBSERVER__SECRET_KEY }}
      pepe: pepe


    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup SSH key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.EC2_SSH_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          ssh-keyscan -H "${{ secrets.EC2_HOST }}" >> ~/.ssh/known_hosts 2>/dev/null || true

      - name: Deploy ETL/Airflow to EC2
        run: |
          ssh ${{ secrets.EC2_USER }}@${{ secrets.EC2_HOST }} << 'EOF'
            set -e

            IMAGE="${{ secrets.DOCKERHUB_USERNAME }}/${{ env.IMAGE_NAME }}:latest"

            sudo mkdir -p /opt/sp500
            cd /opt/sp500

            if ! sudo systemctl is-active --quiet docker; then
              sudo systemctl start docker || true
            fi

            sudo docker network create airflow-net 2>/dev/null || true

            # --- Postgres (volumen persistente) ---
            if sudo docker ps -a --format '{{.Names}}' | grep -q '^airflow-postgres$'; then
              sudo docker stop airflow-postgres || true
              sudo docker rm airflow-postgres || true
            fi

            sudo docker run -d \
              --name airflow-postgres \
              --network airflow-net \
              --restart unless-stopped \
              -e POSTGRES_USER=airflow \
              -e POSTGRES_PASSWORD=airflow \
              -e POSTGRES_DB=airflow \
              -v airflow_pgdata:/var/lib/postgresql/data \
              -p 5432:5432 \
              postgres:15

            # --- Stop/RM contenedores ETL/Airflow ---
            for c in sp500-etl airflow-scheduler airflow-triggerer; do
              if sudo docker ps -a --format '{{.Names}}' | grep -q "^${c}$"; then
                sudo docker stop "${c}" || true
                sudo docker rm "${c}" || true
              fi
            done

            # --- Limpieza fuerte (SIN volúmenes) ---
            sudo docker container prune -f || true
            sudo docker image prune -a -f || true
            sudo docker builder prune -a -f || true

            # --- Pull image nueva ---
            sudo docker pull "${IMAGE}"

            AIRFLOW_DB_URI="postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow"

            # Migración DB
            sudo docker run --rm \
              --network airflow-net \
              -e AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="${AIRFLOW_DB_URI}" \
              -e AIRFLOW__CORE__FERNET_KEY="${{ vars.AIRFLOW__CORE__FERNET_KEY }}" \
              -e AIRFLOW__WEBSERVER__SECRET_KEY="${{ vars.AIRFLOW__WEBSERVER__SECRET_KEY }}" \
              -e AIRFLOW__CORE__EXECUTOR=LocalExecutor \
              -e SP500_URL="https://raw.githubusercontent.com/lucianabuzzo/sp500-analytics/main/data/sp500_companies.csv" \
              "${IMAGE}" airflow db migrate || true

            # Webserver
            sudo docker run -d \
              --name sp500-etl \
              --network airflow-net \
              --restart unless-stopped \
              -p 8080:8080 \
              --log-driver awslogs \
              --log-opt awslogs-region=${{ vars.AWS_DEFAULT_REGION }} \
              --log-opt awslogs-group=/sp500/etl \
              --log-opt awslogs-stream=sp500-etl-webserver \
              -e AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="${AIRFLOW_DB_URI}" \
              -e AIRFLOW__CORE__FERNET_KEY="${{ vars.AIRFLOW__CORE__FERNET_KEY }}" \
              -e AIRFLOW__WEBSERVER__SECRET_KEY="${{ vars.AIRFLOW__WEBSERVER__SECRET_KEY }}" \
              -e AWS_DB_USER=${{ vars.AWS_DB_USER }} \
              -e AWS_DB_PASSWORD=${{ vars.AWS_DB_PASSWORD }} \
              -e AWS_DB_HOST=${{ vars.AWS_DB_HOST }} \
              -e AWS_DB_PORT=${{ vars.AWS_DB_PORT }} \
              -e AWS_DB_NAME=${{ vars.AWS_DB_NAME }} \
              -e AWS_ACCESS_KEY_ID=${{ vars.AWS_ACCESS_KEY_ID }} \
              -e AWS_SECRET_ACCESS_KEY=${{ vars.AWS_SECRET_ACCESS_KEY }} \
              -e AWS_DEFAULT_REGION=${{ vars.AWS_DEFAULT_REGION }} \
              -e AWS_REGION=${{ vars.AWS_DEFAULT_REGION }} \
              -e S3_BUCKET=${{ vars.S3_BUCKET }} \
              -e AIRFLOW__CORE__EXECUTOR=LocalExecutor \
              -e SP500_URL="https://raw.githubusercontent.com/lucianabuzzo/sp500-analytics/main/data/sp500_companies.csv" \
              -e ENVIRONMENT=prod \
              "${IMAGE}" webserver

            # Scheduler
            sudo docker run -d \
              --name airflow-scheduler \
              --network airflow-net \
              --restart unless-stopped \
              --log-driver awslogs \
              --log-opt awslogs-region=${{ vars.AWS_DEFAULT_REGION }} \
              --log-opt awslogs-group=/sp500/etl \
              --log-opt awslogs-stream=sp500-etl-scheduler \
              -e AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="${AIRFLOW_DB_URI}" \
              -e AIRFLOW__CORE__FERNET_KEY="${{ vars.AIRFLOW__CORE__FERNET_KEY }}" \
              -e AIRFLOW__WEBSERVER__SECRET_KEY="${{ vars.AIRFLOW__WEBSERVER__SECRET_KEY }}" \
              -e AIRFLOW__CORE__EXECUTOR=LocalExecutor \
              -e AWS_ACCESS_KEY_ID=${{ vars.AWS_ACCESS_KEY_ID }} \
              -e AWS_SECRET_ACCESS_KEY=${{ vars.AWS_SECRET_ACCESS_KEY }} \
              -e AWS_DEFAULT_REGION=${{ vars.AWS_DEFAULT_REGION }} \
              -e AWS_REGION=${{ vars.AWS_DEFAULT_REGION }} \
              -e S3_BUCKET=${{ vars.S3_BUCKET }} \
              -e SP500_URL="https://raw.githubusercontent.com/lucianabuzzo/sp500-analytics/main/data/sp500_companies.csv" \
              -e ENVIRONMENT=prod \
              "${IMAGE}" scheduler

            # Triggerer
            sudo docker run -d \
              --name airflow-triggerer \
              --network airflow-net \
              --restart unless-stopped \
              --log-driver awslogs \
              --log-opt awslogs-region=${{ vars.AWS_DEFAULT_REGION }} \
              --log-opt awslogs-group=/sp500/etl \
              --log-opt awslogs-stream=sp500-etl-triggerer \
              -e AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="${AIRFLOW_DB_URI}" \
              -e AIRFLOW__CORE__FERNET_KEY="${{ vars.AIRFLOW__CORE__FERNET_KEY }}" \
              -e AIRFLOW__WEBSERVER__SECRET_KEY="${{ vars.AIRFLOW__WEBSERVER__SECRET_KEY }}" \
              -e AIRFLOW__CORE__EXECUTOR=LocalExecutor \
              -e AWS_ACCESS_KEY_ID=${{ vars.AWS_ACCESS_KEY_ID }} \
              -e AWS_SECRET_ACCESS_KEY=${{ vars.AWS_SECRET_ACCESS_KEY }} \
              -e AWS_DEFAULT_REGION=${{ vars.AWS_DEFAULT_REGION }} \
              -e AWS_REGION=${{ vars.AWS_DEFAULT_REGION }} \
              -e S3_BUCKET=${{ vars.S3_BUCKET }} \
              -e SP500_URL="https://raw.githubusercontent.com/lucianabuzzo/sp500-analytics/main/data/sp500_companies.csv" \
              -e ENVIRONMENT=prod \
              "${IMAGE}" triggerer
          EOF

  # === DEPLOY DE STREAMLIT EN EC2 ===
  deploy-streamlit-to-ec2:
    environment: prod
    name: Deploy Streamlit to EC2
    needs: build-and-push-streamlit
    runs-on: ubuntu-latest

    env:
      AWS_ACCESS_KEY_ID: ${{ vars.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ vars.AWS_SECRET_ACCESS_KEY }}
      AWS_DEFAULT_REGION: ${{ vars.AWS_DEFAULT_REGION }}
      S3_BUCKET: ${{ vars.S3_BUCKET }}

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup SSH key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.EC2_SSH_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          ssh-keyscan -H "${{ secrets.EC2_HOST }}" >> ~/.ssh/known_hosts 2>/dev/null || true

      - name: Deploy Streamlit to EC2
        run: |
          ssh ${{ secrets.EC2_USER }}@${{ secrets.EC2_HOST }} << 'EOF'
            set -e

            IMAGE="${{ secrets.DOCKERHUB_USERNAME }}/sp500-streamlit:latest"

            sudo mkdir -p /opt/sp500
            cd /opt/sp500

            if ! sudo systemctl is-active --quiet docker; then
              sudo systemctl start docker || true
            fi

            # --- liberar puerto 8501 (cualquier contenedor que lo esté usando) ---
            for id in $(sudo docker ps --filter "publish=8501" -q); do
              sudo docker stop "$id" || true
              sudo docker rm "$id" || true
            done

            # si existían por nombre
            sudo docker stop sp500-streamlit streamlit-app 2>/dev/null || true
            sudo docker rm sp500-streamlit streamlit-app 2>/dev/null || true

            # --- limpieza fuerte (sin volúmenes) ---
            sudo docker container prune -f || true
            sudo docker image prune -a -f || true
            sudo docker builder prune -a -f || true

            sudo docker pull "${IMAGE}"

            sudo docker run -d \
              --name sp500-streamlit \
              --restart unless-stopped \
              -p 8501:8501 \
              --log-driver awslogs \
              --log-opt awslogs-region=${{ vars.AWS_DEFAULT_REGION }} \
              --log-opt awslogs-group=/sp500/etl \
              --log-opt awslogs-stream=sp500-streamlit \
              -e AWS_ACCESS_KEY_ID=${{ vars.AWS_ACCESS_KEY_ID }} \
              -e AWS_SECRET_ACCESS_KEY=${{ vars.AWS_SECRET_ACCESS_KEY }} \
              -e AWS_DEFAULT_REGION=${{ vars.AWS_DEFAULT_REGION }} \
              -e AWS_REGION=${{ vars.AWS_DEFAULT_REGION }} \
              -e S3_BUCKET=${{ vars.S3_BUCKET }} \
              "${IMAGE}"
          EOF

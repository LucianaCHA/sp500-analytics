name: Docker CI/CD

on:
  push:
    branches:
      - main
    paths:
      - 'airflow/**'
      - 'dags/**'
      - 'scripts/**'
      - 'infra/**'
      - 'streamlit/**'
      - '**/informe_app.py'
      - 'Dockerfile'
      - 'requirements.txt'
      - 'config.py'
      - 'docker-compose.yml'
      - '.github/workflows/docker.yml'
  workflow_dispatch:

env:
  IMAGE_NAME: sp500-etl
  BUILD_CONTEXT: .
  DOCKERFILE_PATH: ./Dockerfile

jobs:
  build-and-push:
    name: Build and Push Docker Image
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Build Docker image
        run: |
          unset DOCKER_CONTEXT || true

          docker build \
            -f ${{ env.DOCKERFILE_PATH }} \
            -t ${{ secrets.DOCKERHUB_USERNAME }}/${{ env.IMAGE_NAME }}:latest \
            ${{ env.BUILD_CONTEXT }}

      - name: Push Docker image
        run: |
          docker push ${{ secrets.DOCKERHUB_USERNAME }}/${{ env.IMAGE_NAME }}:latest

  deploy-to-ec2:
    name: Deploy to EC2
    needs: build-and-push
    runs-on: ubuntu-latest

    env:
      AWS_DB_USER: ${{ vars.AWS_DB_USER }}
      AWS_DB_PASSWORD: ${{ vars.AWS_DB_PASSWORD }}
      AWS_DB_HOST: ${{ vars.AWS_DB_HOST }}
      AWS_DB_PORT: ${{ vars.AWS_DB_PORT }}
      AWS_DB_NAME: ${{ vars.AWS_DB_NAME }}
      AWS_ACCESS_KEY_ID: ${{ vars.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ vars.AWS_SECRET_ACCESS_KEY }}
      AWS_DEFAULT_REGION: ${{ vars.AWS_DEFAULT_REGION }}
      S3_BUCKET: ${{ vars.S3_BUCKET }}
      AIRFLOW__CORE__FERNET_KEY: ${{ vars.AIRFLOW__CORE__FERNET_KEY }}
      AIRFLOW__WEBSERVER__SECRET_KEY: ${{ vars.AIRFLOW__WEBSERVER__SECRET_KEY }}

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup SSH key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.EC2_SSH_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          ssh-keyscan -H "${{ secrets.EC2_HOST }}" >> ~/.ssh/known_hosts 2>/dev/null || true

      - name: Deploy latest image on EC2
        run: |
          ssh ${{ secrets.EC2_USER }}@${{ secrets.EC2_HOST }} << 'EOF'
            set -e

            IMAGE="${{ secrets.DOCKERHUB_USERNAME }}/${{ env.IMAGE_NAME }}:latest"

            # 1) Carpeta base
            sudo mkdir -p /opt/sp500
            cd /opt/sp500

            # 2) Asegurar Docker
            if ! sudo systemctl is-active --quiet docker; then
              sudo systemctl start docker || true
            fi

            # 3) Red de Docker para que Airflow vea a Postgres por nombre
            sudo docker network create airflow-net 2>/dev/null || true

            # 4) Postgres para Airflow: SIEMPRE recrear, con restart policy
            if sudo docker ps -a --format '{{.Names}}' | grep -q '^airflow-postgres$'; then
              echo "Recreando contenedor airflow-postgres..."
              sudo docker stop airflow-postgres || true
              sudo docker rm airflow-postgres || true
            fi

            sudo docker run -d \
              --name airflow-postgres \
              --network airflow-net \
              --restart unless-stopped \
              -e POSTGRES_USER=airflow \
              -e POSTGRES_PASSWORD=airflow \
              -e POSTGRES_DB=airflow \
              -v airflow_pgdata:/var/lib/postgresql/data \
              -p 5432:5432 \
              postgres:15

            # 5) Detener y borrar contenedor anterior si existe
            if sudo docker ps -a --format '{{.Names}}' | grep -q '^sp500-etl$'; then
              echo "Encontrado contenedor anterior sp500-etl, deteniendo y eliminando..."
              sudo docker stop sp500-etl || true
              sudo docker rm sp500-etl || true
            fi

            # 5.1) Scheduler / Triggerer / Streamlit: stop+rm si existen (así recreás limpio)
            for c in airflow-scheduler airflow-triggerer streamlit-app; do
              if sudo docker ps -a --format '{{.Names}}' | grep -q "^${c}$"; then
                echo "Deteniendo y eliminando contenedor ${c}..."
                sudo docker stop "${c}" || true
                sudo docker rm "${c}" || true
              fi
            done

            # 6) Borrar imagen anterior para liberar espacio (si existe)
            if sudo docker images --format '{{.Repository}}:{{.Tag}}' | grep -q "^${IMAGE}$"; then
              echo "Eliminando imagen anterior ${IMAGE} para liberar espacio..."
              sudo docker rmi "${IMAGE}" || true
            fi

            # 7) Limpiar imágenes y contenedores no usados (sin tocar volúmenes de datos)
            sudo docker container prune -f || true
            sudo docker image prune -f || true

            # 8) Pull de la última imagen del ETL (UNA SOLA VEZ)
            echo "Haciendo pull de la nueva imagen..."
            sudo docker pull "${IMAGE}"

            # 9) URL de conexión de Airflow a Postgres (dentro de la red airflow-net)
            AIRFLOW_DB_URI="postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow"
            echo "Usando AIRFLOW_DB_URI=${AIRFLOW_DB_URI}"

            # 10) Migrar / inicializar la DB de Airflow (idempotente)
            sudo docker run --rm \
              --network airflow-net \
              -e AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="${AIRFLOW_DB_URI}" \
              -e AIRFLOW__CORE__FERNET_KEY="${AIRFLOW__CORE__FERNET_KEY}" \
              -e AIRFLOW__WEBSERVER__SECRET_KEY="${AIRFLOW__WEBSERVER__SECRET_KEY}" \
              "${IMAGE}" \
              airflow db migrate || true

            # 11) Levantar Airflow webserver
            sudo docker run -d \
              --name sp500-etl \
              --network airflow-net \
              --restart unless-stopped \
              -p 8080:8080 \
              --log-driver awslogs \
              --log-opt awslogs-region=${AWS_DEFAULT_REGION} \
              --log-opt awslogs-group=/sp500/etl \
              --log-opt awslogs-stream=sp500-etl-webserver \
              -e AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="${AIRFLOW_DB_URI}" \
              -e AIRFLOW__CORE__FERNET_KEY="${AIRFLOW__CORE__FERNET_KEY}" \
              -e AIRFLOW__WEBSERVER__SECRET_KEY="${AIRFLOW__WEBSERVER__SECRET_KEY}" \
              -e AWS_DB_USER=${AWS_DB_USER} \
              -e AWS_DB_PASSWORD=${AWS_DB_PASSWORD} \
              -e AWS_DB_HOST=${AWS_DB_HOST} \
              -e AWS_DB_PORT=${AWS_DB_PORT} \
              -e AWS_DB_NAME=${AWS_DB_NAME} \
              -e AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \
              -e AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \
              -e AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION} \
              -e S3_BUCKET=${S3_BUCKET} \
              -e ENVIRONMENT=prod \
              "${IMAGE}" webserver

            # 12) Levantar Airflow SCHEDULER
            echo "Levantando Scheduler..."
            sudo docker run -d \
              --name airflow-scheduler \
              --network airflow-net \
              --restart unless-stopped \
              --log-driver awslogs \
              --log-opt awslogs-region=${AWS_DEFAULT_REGION} \
              --log-opt awslogs-group=/sp500/etl \
              --log-opt awslogs-stream=sp500-etl-scheduler \
              -e AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="${AIRFLOW_DB_URI}" \
              -e AIRFLOW__CORE__FERNET_KEY="${AIRFLOW__CORE__FERNET_KEY}" \
              -e AIRFLOW__WEBSERVER__SECRET_KEY="${AIRFLOW__WEBSERVER__SECRET_KEY}" \
              -e AIRFLOW__CORE__EXECUTOR=LocalExecutor \
              -e AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \
              -e AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \
              -e AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION} \
              -e S3_BUCKET=${S3_BUCKET} \
              -e ENVIRONMENT=prod \
              "${IMAGE}" scheduler

            # 13) Levantar Airflow TRIGGERER
            echo "Levantando Triggerer..."
            sudo docker run -d \
              --name airflow-triggerer \
              --network airflow-net \
              --restart unless-stopped \
              --log-driver awslogs \
              --log-opt awslogs-region=${AWS_DEFAULT_REGION} \
              --log-opt awslogs-group=/sp500/etl \
              --log-opt awslogs-stream=sp500-etl-triggerer \
              -e AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="${AIRFLOW_DB_URI}" \
              -e AIRFLOW__CORE__FERNET_KEY="${AIRFLOW__CORE__FERNET_KEY}" \
              -e AIRFLOW__WEBSERVER__SECRET_KEY="${AIRFLOW__WEBSERVER__SECRET_KEY}" \
              -e AIRFLOW__CORE__EXECUTOR=LocalExecutor \
              -e AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \
              -e AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \
              -e AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION} \
              -e S3_BUCKET=${S3_BUCKET} \
              -e ENVIRONMENT=prod \
              "${IMAGE}" triggerer

            # 14) Levantar Streamlit App (override de entrypoint)
            echo "Levantando Streamlit..."
            sudo docker run -d \
              --name streamlit-app \
              --network airflow-net \
              --restart unless-stopped \
              -p 8501:8501 \
              --log-driver awslogs \
              --log-opt awslogs-region=${AWS_DEFAULT_REGION} \
              --log-opt awslogs-group=/sp500/etl \
              --log-opt awslogs-stream=sp500-streamlit \
              -e AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \
              -e AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \
              -e AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION} \
              -e S3_BUCKET=${S3_BUCKET} \
              --entrypoint /bin/bash \
              "${IMAGE}" \
              -lc "streamlit run dashboard/streamlit_app/app.py --server.port 8501 --server.address 0.0.0.0"

          EOF